> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [docker.nsddd.top](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9B%AE%E5%BD%95)

> 链学社致力于打造出区块链去中心化的学习平台

*   [authoropen in new window](http://nsddd.top/)

  

> ❤️💕💕新时代拥抱云原生，云原生具有环境统一、按需付费、即开即用、稳定性强特点。Myblog:[http://nsddd.topopen in new window](http://nsddd.top/)

* * *

*   [k3s 介绍](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s%E4%BB%8B%E7%BB%8D)
*   [k3s 和 k8s 区别](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s%E5%92%8Ck8s%E5%8C%BA%E5%88%AB)
*   [架构](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%9E%B6%E6%9E%84)
*   [Sqlite 和 Dqlite](https://docker.nsddd.top/Cloud-Native-k8s/14.html#sqlite-%E5%92%8C-dqlite)
*   [新版本默认支持 etcd](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%96%B0%E7%89%88%E6%9C%AC%E9%BB%98%E8%AE%A4%E6%94%AF%E6%8C%81-etcd)
*   [在线 [[docs/Cloud-Native-k8s/15# 第 15 节 k3s 补充 | 脚本安装]] k3s](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%9C%A8%E7%BA%BF-docs-cloud-native-k8s-15-%E7%AC%AC15%E8%8A%82-k3s-%E8%A1%A5%E5%85%85-%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85-k3s)
*   [在线安装的解析](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%9C%A8%E7%BA%BF%E5%AE%89%E8%A3%85%E7%9A%84%E8%A7%A3%E6%9E%90)
    *   [安装内容](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%AE%89%E8%A3%85%E5%86%85%E5%AE%B9)
    *   [执行操作](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C)
    *   [指定版本](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%8C%87%E5%AE%9A%E7%89%88%E6%9C%AC)
    *   [指定数据库](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E5%BA%93)
    *   [指定容器运行时](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%8C%87%E5%AE%9A%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6)
*   [离线安装解释](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E8%A7%A3%E9%87%8A)
    *   [步骤](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%AD%A5%E9%AA%A4)
    *   [前提条件](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6)
    *   [Containerd + 手动部署镜像方式](https://docker.nsddd.top/Cloud-Native-k8s/14.html#containerd-%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F%E6%96%B9%E5%BC%8F)
    *   [Docker + 手动部署镜像方式](https://docker.nsddd.top/Cloud-Native-k8s/14.html#docker-%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F%E6%96%B9%E5%BC%8F)
    *   [Containerd + 手动部署镜像方式](https://docker.nsddd.top/Cloud-Native-k8s/14.html#containerd-%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F%E6%96%B9%E5%BC%8F-1)
    *   [Containerd + 私有镜像仓库方式](https://docker.nsddd.top/Cloud-Native-k8s/14.html#containerd-%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E6%96%B9%E5%BC%8F)
    *   [Docker + 私有镜像仓库方式](https://docker.nsddd.top/Cloud-Native-k8s/14.html#docker-%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93%E6%96%B9%E5%BC%8F)
    *   [单结点高可用离线安装](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%8D%95%E7%BB%93%E7%82%B9%E9%AB%98%E5%8F%AF%E7%94%A8%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85)
*   [为 kubelet 设置别名](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E4%B8%BAkubelet-%E8%AE%BE%E7%BD%AE%E5%88%AB%E5%90%8D)
*   [扩展 work 节点](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%89%A9%E5%B1%95work%E8%8A%82%E7%82%B9)
*   [CRI CNI CSI](https://docker.nsddd.top/Cloud-Native-k8s/14.html#cri-cni-csi)
*   [嵌入式数据库高可用](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%AB%98%E5%8F%AF%E7%94%A8)
*   [HA 部署实验](https://docker.nsddd.top/Cloud-Native-k8s/14.html#ha-%E9%83%A8%E7%BD%B2%E5%AE%9E%E9%AA%8C)
*   [卸载 k3s](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%8D%B8%E8%BD%BDk3s)
    *   [针对 docker CRI](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E9%92%88%E5%AF%B9-docker-cri)
*   [k3s 的一些重要目录](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9B%AE%E5%BD%95)
    *   [/var/lib/rancher/k3s](https://docker.nsddd.top/Cloud-Native-k8s/14.html#var-lib-rancher-k3s)
    *   [/etc/rancher](https://docker.nsddd.top/Cloud-Native-k8s/14.html#etc-rancher)
    *   [/var/run](https://docker.nsddd.top/Cloud-Native-k8s/14.html#var-run)
*   [镜像加速](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F)
*   [containerd](https://docker.nsddd.top/Cloud-Native-k8s/14.html#containerd)
    *   [架构图](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%9E%B6%E6%9E%84%E5%9B%BE)
    *   [命令](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%91%BD%E4%BB%A4)
    *   [containerd 的配置管理](https://docker.nsddd.top/Cloud-Native-k8s/14.html#containerd%E7%9A%84%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86)
*   [二进制工具](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%B7%A5%E5%85%B7)
*   [边缘计算](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97)
*   [单节点 SQLite 扩展为 etcd 高可用](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%8D%95%E8%8A%82%E7%82%B9-sqlite-%E6%89%A9%E5%B1%95%E4%B8%BA-etcd-%E9%AB%98%E5%8F%AF%E7%94%A8)
*   [安装脚本](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC)
    *   [理解安装的步骤](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E7%90%86%E8%A7%A3%E5%AE%89%E8%A3%85%E7%9A%84%E6%AD%A5%E9%AA%A4)
    *   [标志和环境变量](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%A0%87%E5%BF%97%E5%92%8C%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F)
    *   [K3s Server/Agent - 常用配置](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-server-agent-%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE)
    *   [K3s Server/Agent - 数据库选项](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-server-agent-%E6%95%B0%E6%8D%AE%E5%BA%93%E9%80%89%E9%A1%B9)
    *   [K3s 安装事项 - 网络选项](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E5%AE%89%E8%A3%85%E4%BA%8B%E9%A1%B9-%E7%BD%91%E7%BB%9C%E9%80%89%E9%A1%B9)
    *   [外部数据库](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%BA%93)
    *   [集群数据存储选项](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E9%80%89%E9%A1%B9)
*   [私有镜像仓库](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93)
*   [安装事项 - 注意事项](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E5%AE%89%E8%A3%85%E4%BA%8B%E9%A1%B9-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9)
*   [K3s 集群升级](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7)
*   [K3s 备份恢复](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D)
*   [K3s 卷和存储](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E5%8D%B7%E5%92%8C%E5%AD%98%E5%82%A8)
*   [K3s 网络相关](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3)
*   [helm(k3s)](https://docker.nsddd.top/Cloud-Native-k8s/14.html#helm-k3s)
*   [K3s 高级选项](https://docker.nsddd.top/Cloud-Native-k8s/14.html#k3s-%E9%AB%98%E7%BA%A7%E9%80%89%E9%A1%B9)
*   [所遇到的问题](https://docker.nsddd.top/Cloud-Native-k8s/14.html#%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98)
*   [END 链接](https://docker.nsddd.top/Cloud-Native-k8s/14.html#end-%E9%93%BE%E6%8E%A5)

[toc]

[#](#k3s介绍) k3s 介绍
------------------

**架构图：**

![](http://sm.nsddd.top/smhow-it-works-k3s.svg)

[#](#k3s和k8s区别) k3s 和 k8s 区别
----------------------------

提示

K3s 是一个独立的服务器，与 K8s 不同，它是 Kubernetes 集群的一部分。K8s 依靠 CRI-O 来整合 Kubernetes 与 CRI（容器运行时接口），而 K3s 使用 CRI-O 与所有支持的容器运行时兼容。K8s 使用 kubelet 来调度容器，但 K3s 使用主机的调度机制来调度容器。

k3s 有比 k8s 更严格的安全部署，因为其攻击面小。k3s 的另一个优势是，它可以减少安装、运行或更新 Kubernetes 集群所需的依赖性和步骤。

如何封装 k8s –> k3s 中

原理就是，将 `K8S` 的相关组件封装到 `K3s` 的二进制文件中去，然后启动这二进制文件就可以启动一个成熟的 `K8S` 集群。在下面🔽 我们可以看到 `K3s` 和 `K8S` 的架构基本差不多，其中 `k3s-server` 对应这个 `control-plane`，而 `k3s-agent` 对应着 `node` 节点。

可以看到 `k3s` 中使用的默认存储是 `SQLite`(自带)，且默认的网络使用的是 `Flannel`(自带)。当服务端和客户端都启动之后，通过 `Tunnel-Proxy` 这个组件进行通信，通过这个通道去管理网络流量。在 `agent` 节点中，通过 `kubelet` 操作 `contaninerd` 来创建对应 `Pod`。

[#](#架构) 架构
-----------

k3s 架构就是把 k8s 核心组件封装成二进制~

k3s 分为`k3s server` 和 `k3s agent`：

*   k3s server 只有一个进程体
*   k3s agent 分为两个进程体，其中一个是 Contrainerd，负责管理运行容器

> 在下面也可以深刻理解到

**架构详解：**

架构讲解

k3s 算是对 k8s 的架构和生态进行一部分精华和缩进

**单节点架构：**

K3s 单节点集群的架构如下图所示，该集群有一个内嵌 SQLite 数据库的单节点 `K3s server` 。

在这种配置中，每个 `agent` 节点都注册到同一个 `server` 节点。K3s 用户可以通过调用 `server` 节点上的 K3s API 来操作 Kubernetes 资源。

**单节点 `K3s server` 的架构：**

![](http://sm.nsddd.top/sm1660616402558126.png)

**高可用架构：**

虽然单节点 k3s 集群可以满足各种用例，但对于 Kubernetes control-plane 的正常运行至关重要的环境，您可以在高可用配置中运行 K3s。一个高可用 K3s 集群由以下几个部分组成：

*   **`K3s server` 节点** ：两个或更多的`server`节点将为 Kubernetes API 提供服务并运行其他 control-plane 服务
*   **外部数据库** ：与单节点 k3s 设置中使用的嵌入式 `SQLite` 数据存储相反，高可用 K3s 需要挂载一个 `external database` 外部数据库作为数据存储的媒介。

**K3s 高可用架构（非嵌入式架构图）：**

![](http://sm.nsddd.top/sm1660616476551520.png)

**高可用架构（嵌入式架构）：**

> 注意：高可用结构同样可以使用**嵌入式数据库**
> 
> ⚠️ 区别：
> 
> **嵌入数据库是指数据在内存中数据库，英文称为–embedded**，又称 in-memory embedded database，如 H2, HSQL and Derby databases。
> 
> **非嵌入式数据库是指数据在磁盘中的数据库**，如 MariaDB, MySQL and Oracle。

![](http://sm.nsddd.top/smimage-20221117173105788.png)

**固定 `agent` 节点的注册地址：**

在高可用 `K3s server` 配置中，每个节点还必须使用固定的注册地址向 Kubernetes API 注册，注册后， `agent` 节点直接与其中一个 `server` 节点建立连接：

![](http://sm.nsddd.top/sm1660616545857393.svg)

**注册 `agent` 节点：**

`agent` 节点用`k3s agent`进程发起的 websocket 连接注册，连接由作为代理进程一部分运行的客户端负载均衡器维护。

`agent` 将使用节点集群 `secret` 以及随机生成的节点密码向 `K3s server` 注册，密码存储在 `/etc/rancher/node/password`路径下。 `K3s server` 将把各个节点的密码存储为 `Kubernetes secrets`，随后的任何尝试都必须使用相同的密码。节点密码秘密存储在`kube-system`命名空间中，名称使用模板`<host>.node-password.k3s`。

> **注意：**
> 
> *   在 K3s v1.20.2 之前， `K3s server` 将密码存储在`/var/lib/rancher/k3s/server/cred/node-passwd`的磁盘上。
> *   如果您删除了 `agent` 的`/etc/rancher/node`目录，则需要为该 `agent` 重新创建密码文件，或者从 `server` 中删除该条目。
> *   通过使用`--with-node-id`标志启动 `K3s server` 或 agent，可以将唯一的节点 ID 附加到主机名中。

**自动部署的清单：**

位于目录路径`/var/lib/rancher/k3s/server/manifests` 的清单在构建时被捆绑到 K3s 二进制文件中，将由 [rancher/helm-controlleropen in new window](https://github.com/k3s-io/helm-controller#helm-controller) 在运行时安装。

[#](#sqlite-和-dqlite) Sqlite 和 Dqlite
-------------------------------------

我认为我在这里遇到了很多疑惑，关于 Sqlite 和 [Dqliteopen in new window](https://github.com/canonical/dqlite/blob/master/README_CH.md)

dqlite

“dqlite”是 “distributed SQLite” 的简写，即分布式 SQLite。意味着 dqlite 通过网络协议扩展 SQLite ，将应用程序的各个实例连接在一起，让它们作为一个高可用的集群，而不依赖外部数据库。

我希望 runtime 可以实现 multi-master ，同样支持的嵌入式和外部 DB

警告

关于 单结点 扩展为 高可用 状态，或许这并不是一个很容器实现的地方，我们在前面 details 中看到单结点架构和高可用架构的区别，或许我们应该在制作 `runtime` 模块 和 `rootfs` 的时候更倾向于实现 高可用。

[#](#新版本默认支持-etcd) 新版本默认支持 etcd
-------------------------------

提示

从 `v1.19.5+k3s1` 版本开始，K3s 已添加了对嵌入式 etcd 的完全支持。从 v1.19.1 到 v1.19.4 版本只提供了对嵌入式 etcd 的实验性支持。在 K3s v1.19.1 版本中，嵌入式 etcd 取代了实验性的 Dqlite。这是一个突破性的变化。请注意，不支持从实验性 Dqlite 升级到嵌入式 etcd。如果你尝试升级，升级将不会成功，并且数据将会丢失。

嵌入式 etcd (HA) 在速度较慢的磁盘上可能会出现性能问题，例如使用 SD 卡运行的 Raspberry Pi。

⚠️ 注意，如果你使用 docker 作为 runtime，请小心 docker 是不认识 `+` ，如果你希望的到指定版本，请使用 ： `v1.19.5-k3s1`

[#](#在线-docs-cloud-native-k8s-15-第15节-k3s-补充-脚本安装-k3s) 在线 [[docs/Cloud-Native-k8s/15# 第 15 节 k3s 补充 | 脚本安装]] k3s
----------------------------------------------------------------------------------------------------------------

**安装之前请保证 k3s 的目录干净：**

```
 rm -rf /etc/rancher /var/lib/rancher


```

相比较二进制，推荐脚本安装

虽然可以通过下载二进制文件进行服务端和工作节点的运行 (`./k3s server`)，但是一旦我们退出进程，之前创建的节点也就立即销毁了，所以还是建议使用脚本进行安装。

⚠️ 多说几句：之前不是很理解，xian'z

[#](#在线安装的解析) 在线安装的解析
---------------------

### [#](#安装内容) 安装内容

*   `kubectl`、`crictl`、`ctr`
*   `k3s-killall.sh`、`k3s-uninstall.sh`

### [#](#执行操作) 执行操作

*   将 `kubeconfig` 文件写入到 `/etc/rancher/k3s/k3s.yaml` 里面
*   由 `K3s` 安装的 `kubectl` 工具将自动使用该文件的配置来运行
*   其他机器可以通过复制这个配置文件并修改 `server` 地址来操作 `K3s` 集群

### [#](#指定版本) 指定版本

**我们前面默认安装最新版，或许我们可以指定版本安装：**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.25.3 sh -


```

### [#](#指定数据库) 指定数据库

场景

![](http://sm.nsddd.top/smimage-20221124193104746.png)

**以 MySQL 为例：**

```
curl -sfL https://get.k3s.io | sh -s - server --datastore-endpoint='mysql://admin:Rancher2019k3s@tcp(k3s-mysql.csrskwupj33i.ca-central-1.rds.amazonaws.com:3306)/k3sdb'



```

**任意节点查看 node：**

### [#](#指定容器运行时) 指定容器运行时

**运行时：**

```
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--docker" sh -


curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn | INSTALL_K3S_EXEC="--docker"  sh -


```

> 这样我们可以使用 docker 来管理 k3s

<table><thead><tr><th>Flag</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td><code>--docker</code></td><td>N/A</td><td>用 docker 代替 containerd</td></tr><tr><td><code>--container-runtime-endpoint</code> value</td><td>N/A</td><td>禁用嵌入式 containerd，使用替代的 CRI 实现。</td></tr><tr><td><code>--pause-image</code> value</td><td>"docker.io/rancher/pause:3.1"</td><td>针对 containerd 或 Docker 的自定义 pause 镜像</td></tr><tr><td><code>--snapshotter</code> value</td><td>N/A</td><td>覆盖默认的 containerd 快照程序 (默认: "overlayfs")</td></tr><tr><td><code>--private-registry</code> value</td><td>"/etc/rancher/k3s/registries.yaml"</td><td>私有镜像仓库配置文件</td></tr></tbody></table>k3s 安装动态

```
root@ubuntu:/sealos
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 29713  100 29713    0     0   148k      0 --:--:-- --:--:-- --:--:--  149k
[INFO]  Finding release for channel stable
[INFO]  Using v1.25.3+k3s1 as release
[INFO]  Downloading hash rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/v1.25.3-k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/v1.25.3-k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s

root@ubuntu:/sealos
NAME:
   k3s - Kubernetes, but small and simple

USAGE:
   k3s [global options] command [command options] [arguments...]

VERSION:
   v1.25.3+k3s1 (f2585c16)

COMMANDS:
   server           Run management server
   agent            Run node agent
   kubectl          Run kubectl
   crictl           Run crictl
   ctr              Run ctr
   check-config     Run config check
   etcd-snapshot    Trigger an immediate etcd snapshot
   secrets-encrypt  Control secrets encryption and keys rotation
   certificate      Certificates management
   completion       Install shell completion script
   help, h          Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                     (logging) Turn on debug logs [$K3S_DEBUG]
   --data-dir value, -d value  (data) Folder to hold state (default: /var/lib/rancher/k3s or ${HOME}/.rancher/k3s if not root)
   --help, -h                  show help
   --version, -v               print the version



``` 

[#](#离线安装解释) 离线安装解释
-------------------

提醒

下载离线安装脚本：https://get.k3s.io

下载 **k3s** 二进制文件：k3s

下载必要的`images`：

```
wget https://ghproxy.com/https://github.com/k3s-io/k3s/releases/download/v1.25.3%2Bk3s1/k3s-airgap-images-amd64.tar


```

> **These files are available in the [GitHubopen in new window](https://github.com/k3s-io/k3s/) repository**
> 
> ![](http://sm.nsddd.top/smimage-20221109164523589.png)

### [#](#步骤) 步骤

**步骤 1**：部署镜像，本文提供了两种部署方式，分别是**部署私有镜像仓库**和**手动部署镜像**。请在这两种方式中选择一种执行。

**步骤 2**：安装 K3s，本文提供了两种安装方式，分别是**单节点安装**和**高可用安装**。完成镜像部署后，请在这两种方式中选择一种执行。

**离线升级 K3s 版本**：完成离线安装 K3s 后，您还可以通过脚本升级 K3s 版本，或启用自动升级功能，以保持离线环境中的 K3s 版本与最新的 K3s 版本同步。

**请按照以下步骤准备镜像目录和 K3s 二进制文件：**

> 我认为离线安装的重点在于 **K3s 依赖的镜像**部分，因为 K3s 的 "安装脚本" 和 "二进制文件" 只需要下载到对应目录，然后赋予相应的权限即可，非常简单。但 K3s 依赖的镜像的安装方式取决于你使用的是手动部署镜像还是私有镜像仓库，也取决于容器运行时使用的是 `containerd` 还是`docker`。
> 
> 针对不同的组合形式，可以分为以下几种形式来实现离线安装：
> 
> *   Containerd + 手动部署镜像方式
> *   Docker + 手动部署镜像方式
> *   Containerd + 私有镜像仓库方式
> *   Docker + 私有镜像仓库方式

1.  从 [K3s GitHub Releaseopen in new window](https://github.com/rancher/k3s/releases) 页面获取你所运行的 K3s 版本的镜像 tar 文件。(**airgap-images**)
    
2.  将 tar 文件放在`images`目录下，例如：
    
    ```
    sudo mkdir -p /var/lib/rancher/k3s/agent/images/
    sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/
    
    
    ```
    
3.  将 k3s 二进制文件放在 `/usr/local/bin/k3s`路径下，并确保拥有可执行权限。完成后，现在可以转到下面的[安装 K3sopen in new window](https://docs.rancher.cn/docs/k3s/installation/airgap/_index#%E5%AE%89%E8%A3%85-k3s) 部分，开始安装 K3s。
    

### [#](#前提条件) 前提条件

*   在安装 K3s 之前，完成上面的[部署私有镜像仓库 open in new window](https://docs.rancher.cn/docs/k3s/installation/airgap/_index#%E9%83%A8%E7%BD%B2%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93) 或[手动部署镜像 open in new window](https://docs.rancher.cn/docs/k3s/installation/airgap/_index#%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F)，导入安装 K3s 所需要的镜像。
*   从 [releaseopen in new window](https://github.com/rancher/k3s/releases) 页面下载 K3s 二进制文件，K3s 二进制文件需要与离线镜像的版本匹配。将二进制文件放在每个离线节点的 `/usr/local/bin` 中，并确保这个二进制文件是可执行的。
*   下载 K3s 安装脚本：[https://get.k3s.ioopen in new window](https://get.k3s.io/) 。将安装脚本放在每个离线节点的任意地方，并命名为 `install.sh`。

当使用 `INSTALL_K3S_SKIP_DOWNLOAD` 环境变量运行 K3s 脚本时，K3s 将使用本地的脚本和二进制。

提醒 u

您可以在离线环境中执行单节点安装，在一个 server（节点）上安装 K3s，或高可用安装，在多个 server（节点）上安装 K3s。

对安装脚本进行简单的修改（ghproxy），在最后可以看到 安装脚本~

### [#](#containerd-手动部署镜像方式) Containerd + 手动部署镜像方式

展开查看步骤

假设你已经将同一版本的 K3s 的安装脚本 (`k3s-install.sh`)、K3s 的二进制文件 (`k3s`)、K3s 依赖的镜像 (`k3s-airgap-images-amd64.tar`) 下载到了`/root`目录下。

如果你使用的容器运行时为 containerd，在启动 K3s 时，它会检查`/var/lib/rancher/k3s/agent/images/`是否存在可用的镜像压缩包，如果存在，就将该镜像导入到 `containerd` 镜像列表中。所以我们只需要下载 `K3s` 依赖的镜像到`/var/lib/rancher/k3s/agent/images/`目录，然后启动 `K3s` 即可。

**1. 导入镜像到 `containerd` 的镜像列表：**

```
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp /root/k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/


```

**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/


```

**3. 安装 K3s**

```
INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh


``` 演示

```
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
images  k3s  k3s-install.sh  Kubefile  sealer-runtime-demo
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
[INFO]  Skipping k3s download and verify
[INFO]  Skipping installation of SELinux RPM
[INFO]  Skipping /usr/local/bin/kubectl symlink to k3s, command exists in PATH at /usr/bin/kubectl
[INFO]  Skipping /usr/local/bin/crictl symlink to k3s, command exists in PATH at /usr/bin/crictl
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink from /etc/systemd/system/multi-user.target.wants/k3s.service to /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
Failed to restart k3s.service: Unit is not loaded properly: Invalid argument.
See system logs and 'systemctl status k3s.service' for details.
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
NAME:
   k3s - Kubernetes, but small and simple

USAGE:
   k3s [global options] command [command options] [arguments...]

VERSION:
   v1.25.3+k3s1 (f2585c16)

COMMANDS:
   server           Run management server
   agent            Run node agent
   kubectl          Run kubectl
   crictl           Run crictl
   ctr              Run ctr
   check-config     Run config check
   etcd-snapshot    Trigger an immediate etcd snapshot
   secrets-encrypt  Control secrets encryption and keys rotation
   certificate      Certificates management
   completion       Install shell completion script
   help, h          Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                     (logging) Turn on debug logs [$K3S_DEBUG]
   --data-dir value, -d value  (data) Folder to hold state (default: /var/lib/rancher/k3s or ${HOME}/.rancher/k3s if not root)
   --help, -h                  show help
   --version, -v               print the version



```

**验证：**

```
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
IMAGE                                    TAG                 IMAGE ID            SIZE
k0sproject/k0s                           latest              6adc65a599f7a       253MB
nginx                                    latest              76c69feac34e8       142MB
registry                                 2.7.1               0d0107588605f       25.7MB
sea.hub:5000/calico/apiserver            v3.22.1             b7dd079a4ed76       129MB
sea.hub:5000/calico/cni                  v3.22.1             2a8ef6985a3e5       236MB
sea.hub:5000/calico/kube-controllers     v3.22.1             c0c6672a66a59       132MB
sea.hub:5000/calico/node                 v3.22.1             7a71aca7b60fc       198MB
sea.hub:5000/calico/pod2daemon-flexvol   v3.22.1             17300d20daf93       19.7MB
sea.hub:5000/calico/typha                v3.22.1             f822f80398b9a       127MB
sea.hub:5000/coredns                     1.7.0               bfe3a36ebd252       45.2MB
sea.hub:5000/etcd                        3.4.13-0            0369cf4303ffd       253MB
sea.hub:5000/kube-apiserver              v1.19.8             9ba91a90b7d1b       119MB
sea.hub:5000/kube-controller-manager     v1.19.8             213ae7795128d       111MB
sea.hub:5000/kube-proxy                  v1.19.8             ea03182b84a23       118MB
sea.hub:5000/kube-scheduler              v1.19.8             919a3f36437dc       46.5MB
sea.hub:5000/pause                       3.2                 80d28bedfe5de       683kB
sea.hub:5000/tigera/operator             v1.25.3             648350e58702c       128MB
[root@iZbp1evo5cnwagauz3w188Z k3s-offline]
NAMESPACE          NAME                                              READY   STATUS    RESTARTS   AGE
calico-apiserver   calico-apiserver-64f668766b-dv2xk                 1/1     Running   2          4d17h
calico-apiserver   calico-apiserver-64f668766b-k49gx                 1/1     Running   2          4d17h
calico-system      calico-kube-controllers-69dfd59986-mq7cv          1/1     Running   0          4d17h
calico-system      calico-node-pg47k                                 1/1     Running   0          4d17h
calico-system      calico-typha-84f56b949f-t95jk                     1/1     Running   0          4d17h
default            myapp                                             0/3     Pending   0          4d14h
kube-system        coredns-55bcc669d7-74xb2                          1/1     Running   0          4d17h
kube-system        coredns-55bcc669d7-jdkj2                          1/1     Running   0          4d17h
kube-system        etcd-izbp1evo5cnwagauz3w188z                      1/1     Running   0          4d17h
kube-system        kube-apiserver-izbp1evo5cnwagauz3w188z            1/1     Running   0          4d17h
kube-system        kube-controller-manager-izbp1evo5cnwagauz3w188z   1/1     Running   0          4d17h
kube-system        kube-proxy-ssr6t                                  1/1     Running   0          4d17h
kube-system        kube-scheduler-izbp1evo5cnwagauz3w188z            1/1     Running   0          4d17h
tigera-operator    tigera-operator-7cdb76dd8b-ltbbs                  1/1     Running   10         4d17h


``` 

### [#](#docker-手动部署镜像方式) Docker + 手动部署镜像方式

展开查看步骤

假设你已经将同一版本的 K3s 的安装脚本 (`k3s-install.sh`)、K3s 的二进制文件 (`k3s`)、K3s 依赖的镜像 (`k3s-airgap-images-amd64.tar`) 下载到了`/root`目录下。

与 `containerd` 不同，使用 docker 作为容器运行时，启动 `K3s` 不会导入 `/var/lib/rancher/k3s/agent/images/`目录下的镜像。所以在启动 `K3s` 之前我们需要将 `K3s` 依赖的镜像手动导入到 `docker` 镜像列表中。

**1. 导入镜像到 `docker` 的镜像列表：**

```
sudo docker load -i /root/k3s-airgap-images-amd64.tar


```

**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/


```

**3. 安装 K3s**

```
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='--docker' /root/k3s-install.sh


``` 

### [#](#containerd-手动部署镜像方式-1) Containerd + 手动部署镜像方式

展开查看步骤

假设你已经将同一版本的 K3s 的安装脚本 (`k3s-install.sh`)、K3s 的二进制文件 (`k3s`)、K3s 依赖的镜像 (`k3s-airgap-images-amd64.tar`) 下载到了`/root`目录下。

如果你使用的容器运行时为 containerd，在启动 K3s 时，它会检查`/var/lib/rancher/k3s/agent/images/`是否存在可用的镜像压缩包，如果存在，就将该镜像导入到 `containerd` 镜像列表中。所以我们只需要下载 `K3s` 依赖的镜像到`/var/lib/rancher/k3s/agent/images/`目录，然后启动 `K3s` 即可。

**1. 导入镜像到 `containerd` 的镜像列表：**

```
sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp /root/k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/


```

**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/


```

**3. 安装 K3s**

```
INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh


``` 

### [#](#containerd-私有镜像仓库方式) Containerd + 私有镜像仓库方式

展开查看详细

假设你已经将同一版本的 K3s 的安装脚本 (`k3s-install.sh`)、K3s 的二进制文件 (k3s) 下载到了`/root`目录下。并且 `K3s` 所需要的镜像已经上传到了镜像仓库（本例的镜像仓库地址为：http://192.168.64.44:5000）。K3s 所需的镜像列表可以从 `K3s Release`页面的`k3s-images.txt`获得。

**1. 配置 K3s 镜像仓库**

启动 K3s 默认会从 docker.io 拉取镜像。使用 containerd 容器运行时在离线安装时，我们只需要将镜像仓库地址配置到 docker.io 下的 endpoint 即可，更多配置说明请参考配置 containerd 镜像仓库完全攻略或 [K3s 官方文档 open in new window](https://docs.rancher.cn/docs/k3s/installation/private-registry/_index/)：

```
sudo mkdir -p /etc/rancher/k3s
sudo cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
"docker.io":
endpoint:
- "http://192.168.64.44:5000"
- "https://registry-1.docker.io"
EOF


```

**2. 将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/


```

**3. 安装 K3s**

```
INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s-install.sh


```

> 稍等片刻，即可查看到 K3s 已经成功启动：

### [#](#docker-私有镜像仓库方式) Docker + 私有镜像仓库方式

展开查看详细

假设你已经将同一版本的 K3s 的安装脚本 (k3s-install.sh)、K3s 的二进制文件(k3s) 下载到了 / root 目录下。并且 K3s 所需要的镜像已经上传到了镜像仓库（本例的镜像仓库地址为：http://192.168.64.44:5000）。K3s 所需的镜像列表可以从 K3s Release 页面的 k3s-images.txt 获得。

**1. 配置 K3s 镜像仓库**

Docker 不支持像 containerd 那样可以通过修改 docker.io 对应的 endpoint（默认为 https://registry-1.docker.io）来间接修改默认镜像仓库的地址。但在 Docker 中可以通过配置 registry-mirrors 来实现从其他镜像仓库中获取 K3s 镜像。这样配置之后，会先从 registry-mirrors 配置的地址拉取镜像，如果获取不到才会从默认的 docker.io 获取镜像，从而满足了我们的需求。

```
cat >> /etc/docker/daemon.json <<EOF
{
"registry-mirrors": ["http://192.168.64.44:5000"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker


```

**2、将 K3s 安装脚本和 K3s 二进制文件移动到对应目录并授予可执行权限**

```
sudo chmod a+x /root/k3s /root/k3s-install.sh
sudo cp /root/k3s /usr/local/bin/


```

**3. 安装 k3s：**

```
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='--docker' /root/k3s-install.sh


``` 

### [#](#单结点高可用离线安装) 单结点高可用离线安装

**提供要从 server 节点卸载 K3s，和需要从 agent 结点卸载 K3s，推荐使用高可用安装，关于单结点迁移到高可用状态可参考 [🧷 这篇文章 open in new window](https://mp.weixin.qq.com/s/Yax2m2uFw2d4lo5sybHsCw)：**

```
INSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh


```

然后，要选择添加其他 agent，请在每个 agent 节点上执行以下操作。注意将 `myserver` 替换为 server 的 IP 或有效的 DNS，并将 `mynodetoken` 替换 server 节点的 token，通常在`/var/lib/rancher/k3s/server/node-token`。

```
INSTALL_K3S_SKIP_DOWNLOAD=true K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken ./install.sh


```

```
curl -sfL https://get.k3s.io | sh -s - server \
  --datastore-endpoint='mysql://username:password@tcp(hostname:3306)/database-name'


```

您需要调整安装命令，以便指定`INSTALL_K3S_SKIP_DOWNLOAD=true`并在本地运行安装脚本。您还将利用`INSTALL_K3S_EXEC='args'`为 k3s 提供其他参数。

由于在离线环境中无法使用`curl`命令进行安装，所以您需要参考以下示例，将这条命令行修改为离线安装：

```
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server' K3S_DATASTORE_ENDPOINT='mysql://username:password@tcp(hostname:3306)/database-name' ./install.sh


```

[#](#为kubelet-设置别名) 为 kubelet 设置别名
----------------------------------

`k3s` 安装之后内置了一个 `kubectl` 的子命令，我们通过执行 `k3s kubectl` 命令来调用它，其功能和使用方式都和 `k8s` 的 `kubectl` 命令是一致。为了我们更加方便的使用，可以设置一个 `alias` 别名或者创建一个软连接达到命令的无缝使用。

```
alias kubectl='k3s kubectl'


ln -sf /usr/bin/kubectl /usr/local/bin/k3s


source <(kubectl completion bash)


```

配置完成之后，就可以使用 `kubectl` 来操作集群机器了。通过运行如下命令，可以查看 `kube-system` 名称空间中运行的 `pod` 列表。我们发现并没有运行 `apiserver`、`scheduler`、`kube-proxy` 以及 `flannel` 等组件，因为这些都已经内嵌到了 `k3s` 进程中了。另外 `k3s` 已经给我们默认部署运行了 `traefik ingress`、`metrics-server` 等服务，不需要再额外安装了。

一个很不成熟的想法

或许你也可以将 docker 作为 CRI ， 我不建议那么做，或许有一个折中的方法（也是设置别名）

```
alias docker='k3s crictl'


```

[#](#扩展work节点) 扩展 work 节点
-------------------------

K3s 提供了一个安装脚本，可以方便地将其作为服务安装在基于 systemd 或 openrc 的系统上。该脚本可在 [https://get.k3s.ioopen in new window](https://get.k3s.io/) 获得。要使用这种方法安装 K3s，只需运行：

```
curl -sfL https://get.k3s.io | sh -


```

运行此安装后：

*   K3s 服务将被配置为在节点重启后或进程崩溃或被杀死时自动重启。
*   将安装其他实用程序，包括 `kubectl`、`crictl`、`ctr`、`k3s-killall.sh` 和 `k3s-uninstall.sh`。
*   [kubeconfigopen in new window](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) 文件将写入到 `/etc/rancher/k3s/k3s.yaml`，由 K3s 安装的 kubectl 将自动使用该文件。

要在 Worker 节点上安装并将它们添加到集群，请使用 `K3S_URL` 和 `K3S_TOKEN` 环境变量运行安装脚本。以下示例演示了如何加入 Worker 节点：

```
curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -


```

设置 `K3S_URL` 参数会使 K3s 以 Worker 模式运行。K3s Agent 将注册到在 URL 上监听的 K3s Server。`K3S_TOKEN` 使用的值存储在 Server 节点上的 `/var/lib/rancher/k3s/server/node-token` 中。

注意：每台主机必须具有唯一的主机名。如果你的计算机没有唯一的主机名，请传递 `K3S_NODE_NAME` 环境变量，并为每个节点提供一个有效且唯一的主机名。

[#](#cri-cni-csi) CRI CNI CSI
-----------------------------

**网络**：

*   因为 `k3s` 已经内置了 `Traefik` 组件，不需要再单独安装 `ingress controller` 了，直接创建 `Ingress` 即可。其中 `192.168.xxx.xxx` 为 `master` 节点的 `IP`，由于我们没有 `DNS` 解析，因此可以通过配置 `/etc/hosts` 文件进行静态配置，之后就可以通过域名来访问我们的服务了。

**网络：**

*   因为 `k3s` 已经内置了 `Flannel` 网络插件，默认使用 `VXLAN` 后端，默认 `IP` 段为 `10.42.0.0/16`。内置的 `Flannel` 除了 `VXLAN` 还支持 `ipsec`、`host-gw` 以及 `wireguard`。当然除了默认的 `Flannel`，`k3s` 还支持其他 `CNI`，如 `Canal`、`Calico` 等。

**存储：**

*   `k3s` 删除了 `k8s` 内置 `cloud provider` 以及 `storage` 插件，内置了 `Local Path Provider` 来提供存储。而内置 `local path` 存储，只能单机使用，不支持跨主机使用，也不支持存储的高可用。可以通过使用外部的存储插件解决 k3s 存储问题，比如 `Longhorn` 云原生分布式块存储系统。

[#](#嵌入式数据库高可用) 嵌入式数据库高可用
-------------------------

> 在 K3s v1.19.1 中，嵌入式 etcd 取代了实验性的 `Dqlite`。这是一个突破性的变化。请注意，不支持从实验性 Dqlite 升级到嵌入式 etcd。如果你尝试升级，升级将不会成功，并且数据将会丢失。

etcd 使用的共识算法是 `raft`，HA 模式下保证三个 node 开始~

首先，启动一个带有 `cluster-init` 标志的 Server 节点来启用集群和一个令牌，该令牌将作为共享 secret，用于将其他服务器加入集群。

```
curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --cluster-init


```

启动第一台服务器后，使用共享 secret 将第二台和第三台服务器加入集群：

```
curl -sfL https://get.k3s.io | K3S_TOKEN=SECRET sh -s - server --server https://<ip or hostname of server1>:6443


```

[#](#ha-部署实验) HA 部署实验
---------------------

*   `192.168.71.130` server node
*   `192.168.71.131` agent node
*   `192.168.71.132` agent node

**环境：**

```
root@cubmaster01:/workspces
Linux cubmaster01 5.4.0-132-generic 

root@etcnode01:/current
Linux etcnode01 5.4.0-132-generic 

root@cubnode02:/tmp
Linux cubnode02 5.4.0-132-generic 


```

**master 节点 (192.168.71.130)：**

> 如果网络原因，可以直接复制脚本在本地 `install.sh` 文件，然后运行：
> 
> ```
> INSTALL_K3S_MIRROR=cn K3S_NODE_ | sh install.sh 
> 
> 
> ```

```
curl -sfL https://get.k3s.io  | \
    INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubmaster01 \
    K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config \
    INSTALL_K3S_EXEC="--docker" sh -


```

server 详细安装解释

```
[INFO]  Finding release for channel stable
[INFO]  Using v1.23.6+k3s1 as release


[INFO]  Downloading hash https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/k3s
[INFO]  Verifying binary download


[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr


[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh


[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.


[INFO]  systemd: Starting k3s


``` 

**node1 节点 (192.168.71.131)：**

*   [注册 node 节点 open in new window](https://docs.rancher.cn/docs/k3s/architecture/_index/#%E6%B3%A8%E5%86%8C-agent-%E8%8A%82%E7%82%B9)

> **查看 server 的 token：**
> 
> ```
> sudo cat /var/lib/rancher/k3s/server/token
> 
> 
> ```
> 
> **本地加速：**
> 
> ```
> INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubnode01     K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config     K3S_URL=https://192.168.71.130:6443     K3S_TOKEN=K100be446b6ae6fb8f9a551061516fcc2dac8196de80e747026626b9baab34d57be::server:6cf0d1c8342f340cf5e8344e19493cb7  sh install.sh
> 
> 
> ```

```
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=cubnode01 \
    K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config \
    K3S_URL=https://192.168.71.130:6443 \
    K3S_TOKEN=mynodetoken sh -


```

agent 节点安装详细说明

```
[INFO]  Finding release for channel stable
[INFO]  Using v1.23.6+k3s1 as release


[INFO]  Downloading hash https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/k3s
[INFO]  Verifying binary download


[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Creating /usr/local/bin/kubectl symlink to k3s
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s


[INFO]  Creating killall script /usr/local/bin/k3s-agent-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh


[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service
[INFO]  systemd: Enabling k3s-agent unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.


[INFO]  systemd: Starting k3s-agent


``` 

**node2 节点：**

> node1 节点类似操作

[#](#卸载k3s) 卸载 k3s
------------------

**卸载 k3s：**

卸载 k3s

如果您使用安装脚本安装了 K3s，那么在安装过程中会生成一个卸载 K3s 的脚本。

> 卸载 K3s 会删除集群数据和所有脚本。要使用不同的安装选项重新启动集群，请使用不同的标志重新运行安装脚本。

提供要从 server 节点卸载 K3s，和需要从 agent 结点卸载 K3s：

```
/usr/local/bin/k3s-uninstall.sh


```

```
/usr/local/bin/k3s-agent-uninstall.sh


```

**优秀的你肯定会将他清除干净的：**

```
rm -rf /etc/rancher /var/lib/rancher


```

### [#](#针对-docker-cri) 针对 docker CRI

括 docker 等信息一并清理

```
KUBE_SVC='
kubelet
kube-scheduler
kube-proxy
kube-controller-manager
kube-apiserver
'

for kube_svc in ${KUBE_SVC};
do
  
  if [[ `systemctl is-active ${kube_svc}` == 'active' ]]; then
    systemctl stop ${kube_svc}
  fi
  
  if [[ `systemctl is-enabled ${kube_svc}` == 'enabled' ]]; then
    systemctl disable ${kube_svc}
  fi
done


docker stop $(docker ps -aq)


docker rm -f $(docker ps -qa)


docker volume rm $(docker volume ls -q)


for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher;
do
  umount $mount;
done


mv /etc/kubernetes /etc/kubernetes-bak-$(date +"%Y%m%d%H%M")
mv /var/lib/etcd /var/lib/etcd-bak-$(date +"%Y%m%d%H%M")
mv /var/lib/rancher /var/lib/rancher-bak-$(date +"%Y%m%d%H%M")
mv /opt/rke /opt/rke-bak-$(date +"%Y%m%d%H%M")


rm -rf /etc/ceph \
    /etc/cni \
    /opt/cni \
    /run/secrets/kubernetes.io \
    /run/calico \
    /run/flannel \
    /var/lib/calico \
    /var/lib/cni \
    /var/lib/kubelet \
    /var/log/containers \
    /var/log/kube-audit \
    /var/log/pods \
    /var/run/calico \
    /usr/libexec/kubernetes


no_del_net_inter='
lo
docker0
eth
ens
bond
'

network_interface=`ls /sys/class/net`

for net_inter in $network_interface;
do
  if ! echo "${no_del_net_inter}" | grep -qE ${net_inter:0:3}; then
    ip link delete $net_inter
  fi
done


port_list='
80
443
6443
2376
2379
2380
8472
9099
10250
10254
'

for port in $port_list;
do
  pid=`netstat -atlnup | grep $port | awk '{print $7}' | awk -F '/' '{print $1}' | grep -v - | sort -rnk2 | uniq`
  if [[ -n $pid ]]; then
    kill -9 $pid
  fi
done

kube_pid=`ps -ef | grep -v grep | grep kube | awk '{print $2}'`

if [[ -n $kube_pid ]]; then
  kill -9 $kube_pid
fi



sudo iptables --flush
sudo iptables --flush --table nat
sudo iptables --flush --table filter
sudo iptables --table nat --delete-chain
sudo iptables --table filter --delete-chain
systemctl restart docker


``` 

[#](#k3s-的一些重要目录) k3s 的一些重要目录
-----------------------------

k3s 是一种轻量级的 Kubernetes 发行版，它在安装时会创建一些重要的目录，具体如下：

*   /etc/rancher/k3s/：包含 k3s 配置文件和证书。
*   /var/lib/rancher/k3s/：包含 k3s 数据，例如 etcd 数据库、服务注册表、DNS 缓存和节点信息。
*   /var/log/rancher/k3s/：包含 k3s 服务的日志文件。
*   /usr/local/bin/：包含 k3s 的可执行文件，例如 kubectl 和 k3s 服务管理工具。

这些目录的位置可能会根据您使用的安装方式而有所不同。例如，如果您使用的是内置的 SQLite 数据库，那么可能会将数据存储在 `/var/lib/rancher/k3s/data/` 目录下，而不是独立的 etcd 数据库。

提示

您可以将主要的 k3s 二进制文件放在任何您想要的地方。它会将内容写入 `/var/lib/rancher/k3s` 和 `/etc/rancher`，以及 `containerd` 和 `kubelet` 用于非持久文件的正常位置 `/var/run` 下。

可以在 `/var/lib/rancher/k3s` 路径中找到 `db` 目录（SQLite）。

`/usr/local/bin` 目录中有 k3s 的一些关键目录：

```
root@etcnode01:/usr/local/bin
total 66164
drwxr-xr-x  2 root root     4096 Nov 25 04:31 .
drwxr-xr-x 10 root root     4096 Aug 31 06:52 ..
lrwxrwxrwx  1 root root        3 Nov 25 04:31 crictl -> k3s
-rwxr-xr-x  1 root root 67735552 Nov 25 04:31 k3s
-rwxr-xr-x  1 root root     1433 Nov 25 04:31 k3s-agent-uninstall.sh
-rwxr-xr-x  1 root root     2026 Nov 25 04:31 k3s-killall.sh
lrwxrwxrwx  1 root root        3 Nov 25 04:31 kubectl -> k3s


```

Linux 系统下提供 ln 指令来进行文件链接

我们可以看到这里面有很多的连接文件，同样的，其实在 k3s 的 rootfs bin 目录下也是这样的

::: detatils 软连接 & 硬连接 `ln` 指令默认创建的是硬链接，如果加入了`-s`参数，则会生成一个软链接。

**硬连接：**

在 Linux 中，多个文件名指向同一索引节点是存在的，所以硬连接指通过索引节点来进行的连接，即每一个硬链接都是一个指向对应区域的文件。

硬链接的作用是允许一个文件**拥有多个有效路径名**，这样用户就可以建立硬链接到重要文件, 以防止 “误删” 的功能。

只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个链接被删除后，文件的数据块及目录的连接才会被释放，也就是说，文件才会被真正删除。

⚠️ 注意这并不是 `cp` ，这不是重复文件，注意！！！它们只是指向同一个文件的索引。

**软连接：**

![](http://sm.nsddd.top/smimage-20221125211941734.png)

软链接又叫符号链接，这个文件包含了另一个文件的路径名，例如在上图中，`foo.txt` 就是 `bar.txt` 的软连接，`bar.txt` 是实际的文件，`foo.txt`包含的是对于 `bar.txt` 的 inode 的记录。

软连接可以是任意文件或目录，可以链接不同文件系统的文件，在对符号文件进行读或写操作的时候，系统会自动把该操作转换为对源文件的操作，但删除链接文件时，系统仅仅删除链接文件，而不删除源文件本身，这一点类似于 Windows 操作系统下的快捷方式。

> 软链接以 `l` 开头：
> 
> ```
> lrwxrwxrwx  1 root root    1 Nov 25 13:21 x1 -> x
> 
> 
> ```

:::

### [#](#var-lib-rancher-k3s) `/var/lib/rancher/k3s`

```
[root@VM-4-6-centos k3s]# pwd;ls
/var/lib/rancher/k3s
agent  data  server


```

目录解释

**agent 中：**

```
root@cubmaster01:/var/lib/rancher/k3s/agent
client-ca.crt              client-kube-proxy.key     pod-manifests
client-k3s-controller.crt  containerd                server-ca.crt
client-k3s-controller.key  etc                       serving-kubelet.crt
client-kubelet.crt         k3scontroller.kubeconfig  serving-kubelet.key
client-kubelet.key         kubelet.kubeconfig
client-kube-proxy.crt      kubeproxy.kubeconfig


```

**data 中：**

运行的工作目录：

```
root@cubmaster01:/var/lib/rancher/k3s/data
7c994f47fd344e1637da337b92c51433c255b387d207b30b3e0262779457afe4
current


```

**server 中：**

*   `manifests`：位于目录路径`/var/lib/rancher/k3s/server/manifests` 的清单在构建时被捆绑到 K3s 二进制文件中，将由 [rancher/helm-controlleropen in new window](https://github.com/rancher/helm-controller#helm-controller) 在运行时安装。
    
    ```
    root@ubuntu:/var/lib/rancher/k3s/server
    ccm.yaml      local-storage.yaml  rolebindings.yaml
    coredns.yaml  metrics-server      traefik.yaml
    /var/lib/rancher/k3s/server/manifests
    
    
    ```
    
*   `tls`：tls 证书，k3s 证书默认是一年
    
*   `db`：数据库
    
*   `token、node-token、agent-token`：token
    
*   主节点的密码存储：`/var/lib/rancher/k3s/server/cred/node-passwd`
    

### [#](#etc-rancher) `/etc/rancher`

```
[root@VM-4-6-centos rancher]# pwd;ls
/etc/rancher
k3s  node


```

目录解释

**k3s 中：**

*   k3s.yaml：k3s 集群的一些元数据
    
*   registries.yaml：注册表一些信息，镜像加速
    

**node：**

*   `password`：Agent 将使用节点集群 secret 以及随机生成的节点密码向 k3s server 注册存储密码路径

> K3s server 将把各个节点的密码存储为 Kubernetes secrets，随后的任何尝试都必须使用相同的密码。节点密码秘密存储在`kube-system`命名空间中，名称使用模板`<host>.node-password.k3s`

### [#](#var-run) `/var/run`

*   `k3s`：CRI（containerd or docker)
    
    ```
    root@cubmaster01:/var/run/k3s/containerd# ls
    containerd.sock            io.containerd.runtime.v1.linux
    containerd.sock.ttrpc      io.containerd.runtime.v2.task
    io.containerd.grpc.v1.cri
    
    
    ```
    
*   `containerd`
    
    ```
    root@cubmaster01:/var/run/containerd
    containerd.sock        io.containerd.runtime.v1.linux  runc
    containerd.sock.ttrpc  io.containerd.runtime.v2.task   s
    
    
    ```
    

[#](#镜像加速) 镜像加速
---------------

镜像加速配置后，重启服务

```
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"
EOF


```

重启 k3s 使配置生效

```
sudo systemctl restart k3s


sudo crictl info | grep -A 2 "endpoint"

crictl info|grep  -A 5 registry


```

![](http://sm.nsddd.top/smimage-20221031112848849.png)

[#](#containerd) containerd
---------------------------

*   [https://containerd.io/open in new window](https://containerd.io/)

### [#](#架构图) 架构图

![](http://sm.nsddd.top/smimage-20221110202936935.png)

补充 containerd

containerd 从 docker 就开始熟悉的，那么自然从 docker 开始介绍：

![](https://sm.nsddd.top/sm952033-20180520115357747-1796034956.png)

> 在 docker1.8 之前可以使用 `docker -d`。在后面就是 `docker daemon` 。1.11 以后：`docker`、`dockerd`。2015 年后 OCI 成立，`runtime-spec` 制定
> 
> `libcotainer –> runC`
> 
> ```
> dockerd = docker engine + containerd + containerd - shim + runC
> 
> 
> ```
> 
> …….
> 
> 后面 `kubelet` 不支持 `docker` （因为 `docker` 不支持 `CRI`），`kubernetes`使用 `containerd`。`containerd v1.1`后面也支持 `cri` ，

从图中可以看出，docker 对容器的管理和操作基本都是通过 containerd 完成的。 那么，containerd 是什么呢？

> **containerd** 可用作 Linux 和 Windows 的守护进程。它管理其主机系统的整个容器生命周期，从映像传输和存储到容器执行和监督，再到低级存储，再到网络附件等。

**Containerd 是一个工业级标准的容器运行时，它强调简单性、健壮性和可移植性。Containerd 可以在宿主机中管理完整的容器生命周期：容器镜像的传输和存储、容器的执行和管理、存储和网络等。** 详细点说，Containerd 负责干下面这些事情：

*   管理容器的生命周期 (从创建容器到销毁容器)
*   拉取 / 推送容器镜像
*   存储管理 (管理镜像及容器数据的存储)
*   调用 `runC` 运行容器 (与 `runC` 等容器运行时交互)
*   管理容器网络接口及网络

⚠️ 注意：**Containerd 被设计成嵌入到一个更大的系统中，而不是直接由开发人员或终端用户使用。**

![](http://sm.nsddd.top/smimage-20221031142456840.png)

提示

在上面的安装我们知道了可以选择默认的 docker 安装。

### [#](#命令) 命令

![](http://sm.nsddd.top/smcontainerd-docker-k8s-images)

### [#](#containerd的配置管理) containerd 的配置管理

总结

k3s 安装后内置以下 containerd 客户端

*   `ctr` ： 单纯的容器管理
*   `crictl`：从 kubernetes 视角触发，对 POD，容器进行管理。

**k3s 内修改 containerd 的配置步骤：**

*   复制 `/var/lib/rancher/k3s/agent/containerd/config.toml` 为同目录下的新模板
*   修改 `config.toml.tmpl`
*   重启 `k3s` （systemctl restart k3s) 或者 `k3s-agent`（systemctl restart k3s-agent）
*   检查 `/var/lib/rancher/k3s/agent/containerd/config.toml`

**日志：**

```
tail -f /var/lib/rancher/k3s/agent/containerd/containerd.log


```

[#](#二进制工具) 二进制工具
-----------------

K3s 二进制文件包含许多帮助您管理集群的附加工具。

<table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td><code>k3s server</code></td><td>运行 K3s 管理服务器，它还将启动 Kubernetes 控制平面组件，例如 API 服务器、控制器管理器和调度程序。</td></tr><tr><td><code>k3s agent</code></td><td>运行 K3s 节点代理。这将导致 K3s 作为工作节点运行，启动 Kubernetes 节点服务<code>kubelet</code>和<code>kube-proxy</code>.</td></tr><tr><td><code>k3s kubectl</code></td><td>运行嵌入式 <a href="https://kubernetes.io/docs/docs/reference/kubectl/overview/" target="_blank" rel="noopener noreferrer">kubectl<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg>open in new window</a> CLI。如果<code>KUBECONFIG</code>未设置环境变量，这将自动尝试使用在<code>/etc/rancher/k3s/k3s.yaml</code>启动 K3s 服务器节点时创建的配置文件。</td></tr><tr><td><code>k3s crictl</code></td><td>运行嵌入式 <a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md" target="_blank" rel="noopener noreferrer">crictl<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg>open in new window</a>。这是一个用于与 Kubernetes 的容器运行时接口 (CRI) 交互的 CLI。对调试很有用。</td></tr><tr><td><code>k3s ctr</code></td><td>运行嵌入式 <a href="https://github.com/projectatomic/containerd/blob/master/docs/cli.md" target="_blank" rel="noopener noreferrer">ctr<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg>open in new window</a>。这是 containerd 的 CLI，K3s 使用的容器守护进程。对调试很有用。</td></tr><tr><td><code>k3s etcd-snapshot</code></td><td>对 K3s 集群数据进行按需备份并上传到 S3。有关详细信息，请参阅<a href="https://docs.k3s.io/backup-restore#backup-and-restore-with-embedded-etcd-datastore-experimental" target="_blank" rel="noopener noreferrer">备份和还原<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> open in new window</a>。</td></tr><tr><td><code>k3s secrets-encrypt</code></td><td>将 K3s 配置为在将机密存储在集群中时对其进行加密。有关详细信息，请参阅<a href="https://docs.k3s.io/security/secrets-encryption" target="_blank" rel="noopener noreferrer">秘密加密<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> open in new window</a>。</td></tr><tr><td><code>k3s certificate</code></td><td>证书管理</td></tr><tr><td><code>k3s completion</code></td><td>为 k3s 生成 shell 完成脚本</td></tr><tr><td><code>k3s help</code></td><td>显示命令列表或一个命令的帮助</td></tr></tbody></table>

[#](#边缘计算) 边缘计算
---------------

k3s 非常支持边缘计算，CICD 的部署，可以给我们带来更好的体验。

边缘计算是什么？

边缘计算是为应用开发者和服务提供商在网络的边缘侧提供云服务和 IT 环境服务；目标是在靠近数据输入或用户的地方提供计算、存储和网络带宽。

通俗地说：边缘计算本质上是一种服务，就类似于云计算、大数据服务，但这种服务非常靠近用户；为什么要这么近？目的是为了让用户感觉到刷什么内容都特别快。

**提升了 Quick start 成功率：**

我们在交付软件的时候，从以前的给一个 Java 环境到现在需要一个 k8s 环境，k3s 则集成了，提供开箱即用的交互体验，降低软件的资源占用，并且使运维部署更方便。

[#](#单节点-sqlite-扩展为-etcd-高可用) 单节点 SQLite 扩展为 etcd 高可用
-----------------------------------------------------

> 注意：k3s v1.22.2 及更新版本才支持从单节点 k3s 集群转换为内置 etcd 集群

首先你需要有一个单节点的 k3s 集群，本例使用 1 master 节点、1 worker 节点的 k3s 集群。

```
root@cubmaster01:~/.kube
NAME          STATUS   ROLES                  AGE    VERSION
cubnode01     Ready    <none>                 96m    v1.25.4+k3s1
cubmaster01   Ready    control-plane,master   142m   v1.25.4+k3s1


```

**验证集群：**

```
root@cubmaster01:~/.kube
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   145m


```

**将单节点 K3s 集群转换为内置 etcd 高可用集群**

首先，先停止 k3s 服务：

通过使用 `--cluster-init` 标志重新启动你的 `k3s server` 来将其转换为 etcd 集群

```
root@cubmaster01:/workspces/runtime


```

查看 master 节点的角色，来确认是否转换成功：

```
root@cubmaster01:/workspces/runtime
NAME          STATUS   ROLES                       AGE    VERSION
cubmaster01   Ready    control-plane,etcd,master   151m   v1.25.4+k3s1
cubnode01     Ready    <none>                      105m   v1.25.4+k3s1


```

从上面 ROLES 列可以看到，master 节点的角色增加了 etcd，证明已经通过内置 etcd 数据库重新启动了 k3s 集群。

**验证集群：**

```
root@cubmaster01:/workspces/runtime# kubectl get deployment,svc
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   159m


```

**数据库文件：**

```
root@cubmaster01:/workspces/runtime# ls /var/lib/rancher/k3s/server/db/
etcd  state.db.migrated  state.db-shm  state.db-wal


```

[#](#安装脚本) 安装脚本
---------------

k3s 安装脚本

https://get.k3s.io

Maybe you can try my plan, if you don't choose the domestic route, but you are affected by the firewall. Then you can use `https://ghproxy.com/{github-url}`

国内镜像加速~

https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh

```
curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -


``` 

### [#](#理解安装的步骤) 理解安装的步骤

**先决条件：**

*   选择上，两个节点不能有相同的主机名
*   不修改主机名可以通过添加随机后缀或指定主机名

**硬件信息：**

*   操作系统：可以在大多数现代 Linux 系统上运行（我希望可以有一个测试，我们不用再依赖于 kubernetes 的沉重标配了（不低于 4 核）
*   磁盘设备：K3s 的性能取决于数据库的性能 (建议使用 SSD 硬盘)
*   网络相关：K3s Server 节点的入站规则，所有出站流量都是允许的

<table><thead><tr><th>协议</th><th>端口</th><th>源</th><th>描述</th></tr></thead><tbody><tr><td>TCP</td><td>6443</td><td>K3s agent 节点</td><td>Kubernetes API Server</td></tr><tr><td>UDP</td><td>8472</td><td>K3s server 和 agent 节点</td><td>仅对 Flannel VXLAN 需要</td></tr><tr><td>TCP</td><td>10250</td><td>K3s server 和 agent 节点</td><td>Kubelet metrics</td></tr><tr><td>TCP</td><td>2379-2380</td><td>K3s server 节点</td><td>只有嵌入式 etcd 高可用才需要</td></tr></tbody></table>

**安装选项：**

*   [官方安装参数文档 open in new window](https://docs.rancher.cn/docs/k3s/autok3s/_index/)
*   [安装选项示例演示 open in new window](https://github.com/kingsd041/k3s-tutorial/blob/main/03-%E5%AE%89%E8%A3%85-%E8%A6%81%E6%B1%82%E5%8F%8A%E9%80%89%E9%A1%B9/README.md)

<table><thead><tr><th>Environment Variable</th><th>Description</th></tr></thead><tbody><tr><td><code>INSTALL_K3S_EXEC</code></td><td>用于在服务中启动 <code>K3s</code> 的后续子命令</td></tr><tr><td><code>K3S_CONFIG_FILE</code></td><td>指定配置文件的位置</td></tr><tr><td><code>K3S_TOKEN</code></td><td>用于将 <code>server/agent</code> 加入集群的共享 <code>secret</code> 值</td></tr><tr><td><code>K3S_TOKEN_FILE</code></td><td>用于将 <code>server/agent</code> 加入集群的共享 <code>secret</code> 文件</td></tr><tr><td><code>INSTALL_K3S_VERSION</code></td><td>指定下载 <code>K3s</code> 的版本</td></tr><tr><td><code>K3S_TOKEN_FILE</code></td><td>指定 <code>cluster-secret</code>/<code>token</code> 的文件目录</td></tr><tr><td><code>INSTALL_K3S_SKIP_START</code></td><td>将不会启动 <code>K3s</code> 服务</td></tr><tr><td><code>INSTALL_K3S_SKIP_DOWNLOAD</code></td><td>用于离线安装；设置之后不会下载远程工具</td></tr></tbody></table>

```
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--docker" sh -



curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    K3S_TOKEN=rancher-k3s sh -
sudo cat /var/lib/rancher/k3s/server/token


```

**其他说明**

*   运行 `agent` 时还必须设置 `K3S_TOKEN`
*   以 `K3S_` 开头的环境变量将被保留，供 `systemd/openrc` 使用
*   没有明确设置 `exec` 并设置 `K3S_URL` 的话会将命令默认为工作节点

### [#](#标志和环境变量) 标志和环境变量

在整个 k3s 文档学习中，会看到一些选项可以作为命令标志和环境变量传递进来，那该如何使用标志和环境变量呢？

```
curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -s -
curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_EXEC="--flannel-backend none" sh -s -
curl -sfL https://get.k3s.io | \
    sh -s - server --flannel-backend none


```

### [#](#k3s-server-agent-常用配置) K3s Server/Agent - 常用配置

展开查看

```
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    K3S_KUBECONFIG_OUTPUT=/root/.kube/config \
    sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--docker" sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--container-runtime-endpoint containerd" \
    sh -




curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--private-registry xxx" \
    sh -



route -n


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--node-ip=192.168.100.100" \
    sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    K3S_URL=https://192.168.71.130:6443 K3S_TOKEN=xxx \
    INSTALL_K3S_EXEC="--node-ip=192.168.71.131" \
    sh -





curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC="--tls-san 1.1.1.1"  \
    sh -


kubectl get secret k3s-serving -n kube-system -o yaml


scp ci@1.1.1.1:/etc/rancher/k3s/k3s.yaml ~/.kube/config


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--kubelet-arg=max-pods=200' \
    sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--kube-proxy-arg=proxy-mode=ipvs' \
    sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--kube-apiserver-arg=service-node-port-range=40000-50000' \
    sh -







$ curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--data-dir=/opt/k3s-data' \
    sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--disable traefik' \
    sh -


ls /var/lib/rancher/k3s/server/manifests
kubectl get pods -A | grep traefik


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--node-label foo=bar,hello=world \
        --node-taint key1=value1:NoExecute'
    sh -


kubectl describe nodes


``` 

### [#](#k3s-server-agent-数据库选项) K3s Server/Agent - 数据库选项

```
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--datastore-endpoint etcd' \
    sh -
    
 


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--etcd-snapshot-schedule-cron * */5 * * *' \
    sh -


```

### [#](#k3s-安装事项-网络选项) K3s 安装事项 - 网络选项

默认情况下，`K3s` 将以 `flannel` 作为 `CNI` 运行，使用 `VXLAN` 作为默认后端，`CNI` 和默认后端都可以通过参数修改。要启用加密，请使用下面的 `IPSec` 或 `WireGuard` 选项。

```
sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json
{
    "Network": "10.42.0.0/16",
    "EnableIPv6": false,
    "EnableIPv4": true,
    "IPv6Network": "::/0",
    "Backend": {
        "Type": "vxlan"
    }
}


```

<table><thead><tr><th>CLI Flag 和 Value</th><th>描述</th></tr></thead><tbody><tr><td><code>--flannel-backend=vxlan</code></td><td>使用 <code>VXLAN</code> 后端 (默认)</td></tr><tr><td><code>--flannel-backend=host-gw</code></td><td>使用 <code>host-gw</code> 后端</td></tr><tr><td><code>--flannel-backend=ipsec</code></td><td>使用 <code>IPSEC</code> 后端；对网络流量进行加密</td></tr><tr><td><code>--flannel-backend=wireguard</code></td><td>使用 <code>WireGuard</code> 后端；对网络流量进行加密</td></tr></tbody></table>

**配置 Flannel 选项**

这样，我就可以在安装 `K3s` 或者之后修改对应配置文件，来修改 `Flannel` 默认的后端网络配置选项 (重启会覆盖不生效) 了。下面，我们演示下，如何修改为 `host-gw` 模式。

```
curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--flannel-backend=host-gw' \
    sh -


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.100.100:6443 \
    K3S_TOKEN=xxx sh -


route -n
0.0.0.0         172.16.64.1     0.0.0.0         UG    100    0        0 enp0s2
10.42.1.0       172.16.64.9     255.255.255.0   UG    0      0        0 enp0s2


sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json
{
    "Network": "10.42.0.0/16",
    "Backend": {
        "Type": "host-gw"
    }
}


```

**启用 Directrouting 特性**

**Flannel 自身的特性**：当主机在同一子网时，启用 `direct routes`(如 `host-gw`)。`vxlan` 只用于将数据包封装到不同子网的主机上，同子网的主机之间使用 `host-gw`，默认值为 `false`。

要添加我们就不能修改其对应的网络配置文件，因为重新安装或者重启都会把这个配置冲掉 (变成默认配置)，所以需要折中下。我们自建一个网络配置文件，然后在启动的时候执行从哪个配置文件里面加载对应配置。

```
sudo cat /etc/flannel/net-conf.json
{
    "Network": "10.42.0.0/16",
    "Backend": {
        "Type": "vxlan",
        "Directrouting": true
    }
}


curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--flannel-backend=host-gw' \
    sh -


```

**自定义 CNI**

使用 `--flannel-backend=none`(禁用) 运行 `K3s`，然后在安装你选择的 `CNI`。按照 Calico CNI 插件指南 来修改 `Calico` 的 `YAML` 配置文件，在 `container_settings` 部分中允许 `IP` 转发。

```
"container_settings": {
    "allow_ip_forwarding": true
}

- name: CALICO_IPV4POOL_CIDR
  value: "192.168.200.0/24"


sudo cat /etc/cni/net.d/10-canal.conflist



curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--flannel-backend=none \
        --cluster-cidr=192.168.200.0/24"' \
    sh -


kubectl apply -f ./calico.yaml


```

### [#](#外部数据库) 外部数据库

> **理解 Server 节点的安装，以及注册 Agent 节点的步骤！**

**使用外部数据库实现高可用安装**

*   两个或多个`server` 节点
*   零个或多个`agent` 节点
*   外部数据存储 (`Etcd/MySQL/PostgRES`)
*   固定的注册地址 (`LB`)
*   这应该是最适合国内用户的 **K3s HA** 方案： https://mp.weixin.qq.com/s/0Wk2MzfWqMqt8DfUK_2ICA

虽然单节点 `k3s server` 集群可以满足各种用例，但是对于需要稳定运行的重要环境，可以在 `HA` 配置中运行 `K3s`，如何使用外部数据库安装一个高可用的 `K3s` 集群？

<table><thead><tr><th>主机名</th><th>角色</th><th>IP</th></tr></thead><tbody><tr><td>k3s-server-1</td><td>k3s master</td><td>172.31.2.134</td></tr><tr><td>k3s-server-2</td><td>k3s master</td><td>172.31.2.42</td></tr><tr><td>k3s-db</td><td>DB</td><td>172.31.10.251</td></tr><tr><td>k3s-lb</td><td>LB</td><td>172.31.13.97</td></tr><tr><td>k3s-agent</td><td>k3s agent</td><td>172.31.15.130</td></tr></tbody></table>展开查看安装过程

```
docker run --name some-mysql \
    --restart=unless-stopped -p 3306:3306 \
    -e MYSQL_ROOT_PASSWORD=password -d mysql:5.7




curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn sh - server \
    --datastore-endpoint="mysql://root:password@ip:3306/k3s" \
    --tls-san 172.31.13.97



cat >> /etc/nginx.conf <<EOF
worker_processes 4;
worker_rlimit_nofile 40000;

events {
    worker_connections 8192;
}

stream {
    upstream k3s_api {
        least_conn;
        server 172.31.2.134:6443 max_fails=3 fail_timeout=5s;
        server 172.31.2.42:6443 max_fails=3 fail_timeout=5s;
    }
    server {
        listen     6443;
        proxy_pass k3s_api;
    }
}
EOF


docker run -d --restart=unless-stopped \
  -p 6443:6443 \
  -v /etc/nginx.conf:/etc/nginx/nginx.conf \
  nginx:1.14




curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn
    K3S_URL=https://172.31.13.97:6443 K3S_TOKEN=mynodetoken \
    sh -


kubectl get nodes
NAME           STATUS   ROLES                  AGE   VERSION
k3s-server-1   Ready    control-plane,master   68s   v1.20.7+k3s1
k3s-server-2   Ready    control-plane,master   66s   v1.20.7+k3s1


```

**嵌入式 DB 的高可用**

要在这种模式下运行 `K3s`，你必须有**奇数的服务器节点**（raft)，建议从三个节点开始。在嵌入式中，默认使用 `Etcd` 作为高可用的数据库。

```
root@cubnode02:/workspces
    INSTALL_K3S_MIRROR=cn K3S_TOKEN=SECRET \
    sh -s - --cluster-init


root@cubnode02:/workspces
NAME    STATUS  ROLES                      AGE  VERSION
ip-xxx  Ready   control-plane,etcd,master  19h  v1.23.6+k3s1


root@cubnode02:/workspces
    INSTALL_K3S_MIRROR=cn K3S_TOKEN=SECRET \
    sh -s - --server https://<ip-or-host-server>:6443




root@cubnode02:/workspces
    https://172.31.4.43:2379,\
    https://172.31.4.190:2379' \
ETCDCTL_CACERT='/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt' \
ETCDCTL_CERT='/var/lib/rancher/k3s/server/tls/etcd/server-client.crt'\
ETCDCTL_KEY='/var/lib/rancher/k3s/server/tls/etcd/server-client.key' \
ETCDCTL_API=3 etcdctl endpoint status --write-out=table


```

### [#](#集群数据存储选项) 集群数据存储选项

使用 `etcd` 以外的数据存储运行 `K8S` 的能力使 `K3s` 区别于其他 `K8S` 发行版。该功能为 `K8S` 操作者提供了灵活性，可用的数据存储选项允许你选择一个最适合用例的数据存储。

如果你的团队没有操作 `etcd` 的专业知识，可以选择 `MySQL` 或 `PostgreSQL` 等企业级 `SQL` 数据库。如果您需要在 `CI/CD` 环境中运行一个简单的、短暂的集群，可以使用嵌入式 `SQLite` 数据库

如果你想使用外部数据存储，如 `PostgreSQL`、`MySQL` 或 `etcd`，你必须设置 `datastore-endpoint` 参数，以便 `K3s` 知道如何连接到它，也可以指定参数来配置连接的认证和加密。下表总结了这些参数，它们可以作为 `CLI` 标志或环境变量传递。

<table><thead><tr><th>CLI Flag</th><th>环境变量</th><th>描述</th></tr></thead><tbody><tr><td><code>--datastore-endpoint</code></td><td><code>K3S_DATASTORE_ENDPOINT</code></td><td>指定一个 PostgresSQL、MySQL 或 etcd 连接字符串。用于描述与数据存储的连接。这个字符串的结构是特定于每个后端的，详情如下。</td></tr><tr><td><code>--datastore-cafile</code></td><td><code>K3S_DATASTORE_CAFILE</code></td><td>TLS 证书颁发机构（CA）文件，用于帮助确保与数据存储的通信安全。如果你的数据存储通过 TLS 服务请求，使用由自定义证书颁发机构签署的证书，你可以使用这个参数指定该 CA，这样 K3s 客户端就可以正确验证证书。</td></tr><tr><td><code>--datastore-certfile</code></td><td><code>K3S_DATASTORE_CERTFILE</code></td><td>TLS 证书文件，用于对数据存储进行基于客户端证书的验证。要使用这个功能，你的数据存储必须被配置为支持基于客户端证书的认证。如果你指定了这个参数，你还必须指定<code>datastore-keyfile</code>参数。</td></tr><tr><td><code>--datastore-keyfile</code></td><td><code>K3S_DATASTORE_KEYFILE</code></td><td>TLS 密钥文件，用于对数据存储进行基于客户端证书的认证。更多细节请参见前面的<code>datastore-certfile</code>参数。</td></tr></tbody></table>

作为最佳实践，我们建议将这些参数设置为环境变量，而不是命令行参数，这样你的数据库证书或其他敏感信息就不会作为进程信息的一部分暴露出来。

[#](#私有镜像仓库) 私有镜像仓库
-------------------

`K3s` 默认使用 `containerd` 作为容器运行时，所以在 `docker` 上配置镜像仓库是不生效的。`K3s` 镜像仓库配置文件由两大部分组成：`mirrors` 和 `configs`。

*   `Mirrors` 是一个用于定义专用镜像仓库的名称和 `endpoint` 的指令
*   `Configs` 部分定义了每个 `mirror` 的 `TLS` 和证书配置
*   对于每个 `mirror`，你可以定义 `auth` 和 `/` 或 `tls`

`K3s registry` 配置目录为： `/etc/rancher/k3s/registries.yaml`。`K3s` 启动时会检查 `/etc/rancher/k3s/` 中是否存在 `registries.yaml` 文件，并指示 `containerd` 使用文件中定义的镜像仓库。如果你想使用一个私有的镜像仓库，那么你需要在每个使用镜像仓库的节点上以 `root` 身份创建这个文件。

请注意，`server` 节点默认是可以调度的。如果你没有在 `server` 节点上设置污点，那么将在它们上运行工作负载，请确保在每个 `server` 节点上创建 `registries.yaml` 文件。

`containerd` 使用了类似 `K8S` 中 `svc` 与 `endpoint` 的概念，`svc` 可以理解为访问名称，这个名称会解析到对应的 `endpoint` 上。也可以理解 `mirror` 配置就是一个反向代理，它把客户端的请求代理到 `endpoint` 配置的后端镜像仓库。`mirror` 名称可以随意填写，但是必须符合 `IP` 或域名的定义规则。并且可以配置多个 `endpoint`，默认解析到第一个 `endpoint`，如果第一个 `endpoint` 没有返回数据，则自动切换到第二个 `endpoint`，以此类推。

```
mirrors:
  "172.31.6.200:5000":
    endpoint:
      - "http://172.31.6.200:5000"
      - "http://x.x.x.x:5000"
      - "http://y.y.y.y:5000"
  "rancher.ksd.top:5000":
    endpoint:
      - "http://172.31.6.200:5000"
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"

configs:
  "172.31.6.200:5000":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.escapelife.site.cert
      key_file: /home/ubuntu/harbor2.escapelife.site.key
      ca_file: /home/ubuntu/ca.crt

$ sudo systemctl restart k3s.service
$ sudo crictl pull 172.31.6.200:5000/library/alpine
$ sudo crictl pull rancher.ksd.top:5000/library/alpine


```

这里我们介绍下，如何使用 `TLS` 配置。

展开

```
cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "harbor.escapelife.site":
    endpoint:
      - "https://harbor.escapelife.site"
configs:
  "harbor.escapelife.site":
    auth:
      username: admin
      password: Harbor@12345
EOF

sudo systemctl restart k3s


cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "harbor2.escapelife.site":
    endpoint:
      - "https://harbor2.escapelife.site"
configs:
  "harbor2.escapelife.site":
    auth:
      username: admin
      password: Harbor@12345
    tls:
      cert_file: /home/ubuntu/harbor2.escapelife.site.cert
      key_file:  /home/ubuntu/harbor2.escapelife.site.key
      ca_file:   /home/ubuntu/ca.crt
EOF

sudo systemctl restart k3s


cat >> /etc/rancher/k3s/registries.yaml <<EOF
mirrors:
  "docker.io":
    endpoint:
      - "https://fogjl973.mirror.aliyuncs.com"
      - "https://registry-1.docker.io"
EOF

sudo systemctl restart k3s


``` 

`K3s` 将会在 `/var/lib/rancher/k3s/agent/etc/containerd/config.toml` 中为 `containerd` 生成 `config.toml`。如果要对这个文件进行高级设置，你可以在同一目录中创建另一个名为 `config.toml.tmpl` 的文件，此文件将会代替默认设置。

```
cat >> /etc/rancher/k3s/registries.yaml
mirrors:
  "harbor.escapelife.site":
     endpoint:
     - "https://harbor.escapelife.site"
  "harbor2.escapelife.site":
     endpoint:
     - "https://harbor2.escapelife.site"
  "172.31.19.227:5000":
     endpoint:
     - "http://172.31.19.227:5000"
  "docker.io":
     endpoint:
     - "https://fogjl973.mirror.aliyuncs.com"
     - "https://registry-1.docker.io"

configs:
  "harbor.escapelife.site":
     auth:
       username: admin
       password: Harbor@12345

  "harbor2.escapelife.site":
     auth:
       username: admin
       password: Harbor@12345
     tls:
       cert_file: /home/ubuntu/harbor2.escapelife.site.cert
       key_file:  /home/ubuntu/harbor2.escapelife.site.key
       ca_file:   /home/ubuntu/ca.crt


```

[#](#安装事项-注意事项) 安装事项 - 注意事项
---------------------------

**Helm**

*   如果需要使用 `helm` 操作 `K3s` 集群，需要创建 `~/.kube/conf` 目录
*   需要执行 `cp /etc/rancher/k3s/k3s.yaml ~/.kube/config` 命令

**自动部署的清单**

*   将由 `rancher/helm-controller` 在运行时安装
*   目录路径：`/var/lib/rancher/k3s/server/manifests`
*   目录下面的每个 `yaml` 就代表这个一个需要启动的服务

对于我们希望使用的组件，可以在启动的时候禁用默认组件，在手动部署你需要的一些组件 (通常是放到一个指定目录下面，随着服务启动自动拉起)，从而达到灵活使用的目的。

**注册 Agent 节点**

1.  工作节点密码存储：`/etc/rancher/node/password`
2.  主节点的密码存储：`/var/lib/rancher/k3s/server/cred/node-passwd`

在 `agent` 节点运行注册命令，会和 `server` 节点发起 `websocket` 连接，然后会在工作节点上面创建一个随机的密码。然后会拿着这个密码和工作节点的主机名，发送给主节点。然后主节点会将这个信息在保存 (`k8s secrets`) 起来，随后的任何尝试都必须使用相同的密码。

```
sudo cat /etc/rancher/node/password



sudo kubectl get secret k3s2.node-password.k3s -o yaml -n kube-system


sudo tail -200f /var/log/syslog | grep k3s



sudo kubectl delete secret k3s2.node-password.k3s -n kube-system


```

**自定义存储类型**

集群启动之后，默认会启动一个 `local-path` 的组件，用于提供服务挂载存储使用，其默认以 `PVC` 的形式。之后，将其存储在 `/var/lib/rancher/k3s/server/storageclass` 目录下面。

```
sudo kubectl get pods -A


sudo kubectl get storageclass



curl -sfL https://get.k3s.io | \
    INSTALL_K3S_MIRROR=cn \
    INSTALL_K3S_EXEC='--etcd-snapshot-schedule-cron * */5 * * *' \
    sh -


```

[#](#k3s-集群升级) K3s 集群升级
-----------------------

> **手动升级 + 自动升级**

当升级 `K3s` 时，`K3s` 服务会重启或停止，但 `K3s` 容器会继续运行。要停止所有的 `K3s` 容器并重置容器的状态，可以使用 `k3s-killall.sh` 脚本。 `killall` 脚本清理容器、`K3s` 目录和网络组件，同时也删除了 `iptables` 链和所有相关规则。集群数据不会被删除。

**手动升级 - 使用安装脚本升级 K3s**

你可以通过使用安装脚本升级 `K3s`，或者手动安装所需版本的二进制文件。

```
curl -sfL https://get.k3s.io | sh -


curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -


curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL="v1.20" sh -


curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -


```

> 注意和 docker 的区别，关于版本是 `+` 还是 `-`

**手动升级 - 使用二进制文件手动升级 K3s**

你可以通过使用安装脚本升级 `K3s`，或者手动安装所需版本的二进制文件。

```
https://github.com/rancher/k3s/releases


$ mv ./k3s /usr/local/bin/k3s


$ curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL="v1.20" sh -


$ curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -


```

你可以使用 `Rancher` 的 `system-upgrad-controller` 来管理 `K3s` 集群升级。这是一种 `Kubernetes` 原生的集群升级方法。它利用自定义资源定义 (`CRD`)、计划和控制器，根据配置的计划安排升级。

控制器通过监控计划和选择要在其上运行升级 `job` 的节点来调度升级，计划通过标签选择器定义哪些节点应该升级。当一个 `job` 成功运行完成后，控制器会给它运行的节点打上相应的标签。

**自动升级 - 使用二进制文件手动升级 K3s**

*   k3s-upgrade：https://github.com/k3s-io/k3s-upgrade
*   system-upgrade-controller：https://github.com/rancher/system-upgrade-controller

```
kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.6.2/system-upgrade-controller.yaml


```

配置计划

```

apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: server-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/master 
      operator: In
      values:
      - "true"
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.20.4+k3s1


apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: agent-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/master 
      operator: DoesNotExist
  prepare:
    args:
    - prepare
    - server-plan
    image: rancher/k3s-upgrade
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.20.4+k3s1

apiVersion: upgrade.cattle.io/v1
kind: Plan
...
spec:
  ...
  upgrade:
    image: rancher/k3s-upgrade
  channel: https://update.k3s.io/v1-release/channels/stable


``` 

![](http://sm.nsddd.top/smimage-20221126000754469.png)

[#](#k3s-备份恢复) K3s 备份恢复
-----------------------

> **SQLite + etcd + 外部数据存储**

**使用嵌入式 SQLite 数据存储进行备份和恢复**

```
cp -rf /var/lib/rancher/k3s/server/db /opt/db


systemctl stop k3s
rm -rf /var/lib/rancher/k3s/server/db
cp -rf /opt/db /var/lib/rancher/k3s/server/db
systemctl start k3s





sqlite3 /var/lib/rancher/k3s/server/db/state.db
SQLite version 3.22.0 2018-01-22 18:45:57
Enter ".help" for usage hints.
sqlite> .backup "/opt/kine.db"
sqlite> .exit


sudo systemctl stop k3s

sqlite3 /var/lib/rancher/k3s/server/db/state.db
SQLite version 3.22.0 2018-01-22 18:45:57
Enter ".help" for usage hints.
sqlite> .restore '/opt/kine.db'
sqlite> .exit

sudo systemctl start k3s


```

当使用外部数据存储时，备份和恢复操作是在 `K3s` 之外处理的。数据库管理员需要对外部数据库进行备份，或者从快照或转储中进行恢复。我们建议将数据库配置为执行定期快照。

**使用外部数据存储进行备份和恢复**

```
mysqldump -uroot -p --all-databases --master-data > k3s-dbdump.db


systemctl stop k3s
mysql -uroot -p  < k3s-dbdump.db
systemctl start k3s


```

**使用嵌入式 etcd 数据存储进行备份和恢复**

```
--etcd-disable-snapshots       禁用自动etcd快照
--etcd-snapshot-schedule-cron  定时快照的时间点；认值为每12小时触发一次
--etcd-snapshot-retention      保留的快照数量；默认值为5
--etcd-snapshot-dir            保存数据库快照的目录路径
--cluster-reset                忘记所有的对等体；成为新集群的唯一成员
--cluster-reset-restore-path   要恢复的快照文件的路径


```

当 `K3s` 从备份中恢复时，旧的数据目录将被移动到`/var/lib/rancher/k3s/server/db/etcd-old/`。然后 `K3s` 会尝试通过创建一个新的数据目录来恢复快照，然后从一个带有一个 `etcd` 成员的新 `K3s` 集群启动 `etcd`。

```
./k3s server \
    --cluster-reset \
    --cluster-reset-restore-path=<PATH-TO-SNAPSHOT>


```

[#](#k3s-卷和存储) K3s 卷和存储
-----------------------

> **介绍了如何通过 local storage provider 或 Longhorn 来设置持久存储。**

当部署一个需要保留数据的应用程序时，你需要创建持久存储。持久存储允许您从运行应用程序的 `pod` 外部存储应用程序数据。即使应用程序的 `pod` 发生故障，这种存储方式也可以使您维护应用程序数据。

**设置 Local Storage Provider 支持**

`K3s` 自带 `Rancher` 的 `Local Path Provisioner`(`LPP`)，这使得能够使用各自节点上的本地存储来开箱即用地创建 `pvc`。根据用户配置，`LPP` 将自动在节点上创建基于 `hostPath` 的持久卷。它利用了 `K8s` 的 `Local Persistent Volume` 特性引入的特性，但它比 `K8s` 中内置的 `local pv` 特性更简单的解决方案。

pvc.yaml

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-path-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 2Gi


apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: local-path-pvc


``` 

```
kubectl create -f pvc.yaml pod.yaml


kubectl get pv
kubectl get pvc


```

**设置 Longhorn 支持**

`K3s` 支持 `Longhorn`(是 `K8s` 的一个开源分布式块存储系统)。

pvc.yaml

```
$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 2Gi


apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc


``` 

```
kubectl create -f pvc.yaml pod.yaml


kubectl get pv
kubectl get pvc


```

[#](#k3s-网络相关) K3s 网络相关
-----------------------

**CoreDNS**

`CoreDNS` 是在 `agent` 节点启动时部署的。要禁用，请在每台服务器上运行 `--disable coredns` 选项。如果你不安装 `CoreDNS`，你将需要自己安装一个集群 `DNS` 提供商。

```
1.将coredns.yaml保存到其他目录
2.通过 --disable coredns 禁用coredns
3.复制coredns.yaml到/var/lib/rancher/k3s/server/manifests/目录并修改参数


```

**Traefik Ingress Controller**

启动 `server` 时，默认情况下会部署 `Traefik`，对该文件的任何修改都会以类似 `kubectl apply` 的方式自动部署到 `Kubernetes` 中，将使用主机上的 `80` 和 `443` 端口。

```


apiVersion: v1
kind: Secret
metadata:
  name: authsecret
  namespace: default
data:
  users: |2
    YWRtaW46JGFwcjEkLkUweHd1Z0EkUjBmLi85WndJNXZWRFMyR2F2LmtELwoK

---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: traefik-dashboard
spec:
  routes:
    - match: Host(`traefik.example.com`) && (PathPrefix(`/api`) || PathPrefix(`/dashboard`))
      kind: Rule
      services:
        - name: api@internal
          kind: TraefikService
      middlewares:
        - name: auth

---
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: auth
spec:
  basicAuth:
    secret: authsecret 


```

**Service Load Balancer**

`K3s` 提供了一个名为 `Klipper Load Balancer` 的负载均衡器，它可以使用可用的主机端口。允许创建 `LoadBalancer` 类型的 `Service`，但不包括 `LB` 的实现。某些 `LB` 服务需要云提供商，例如 `Amazon EC2`。相比之下，`K3s service LB` 使得可以在没有云提供商的情况下使用 `LB` 服务。

[#](#helm-k3s) helm(k3s)
------------------------

**K3s 与 Helm**

> **`Helm`** **是** **`Kubernetes`** **的包管理工具！**

`Helm` 是 `Kubernetes` 的包管理工具。`Helm Chart` 为 `Kubernetes YAML` 清单文件提供了模板化语法，可以通过 `Helm` 安装对应的 `chart`。`K3s` 不需要任何特殊的配置就可以使用 `Helm` 命令行工具。

**自动部署 Helm charts**

在 `/var/lib/rancher/k3s/server/manifests` 中找到的任何 `Kubernetes` 清单将以类似 `kubectl apply` 的方式自动部署到 `K3s`。以这种方式部署的 `manifests` 是作为 `AddOn` 自定义资源来管理的。你会发现打包组件的 `AddOns`，如 `CoreDNS`、`Local-Storage` 等。`AddOns` 是由部署控制器自动创建的，并根据它们在 `manifests` 目录下的文件名命名。

```
$ kubectl get addon -A


https://github.com/rancher/helm-controller/


```

![](http://sm.nsddd.top/smimage-20221126114506014.png)

**使用 Helm CRD**

`HelmChart CRD` 捕获了大多数你通常会传递给 `helm` 命令行工具的选项。下面是一个例子，说明如何从默认的 `Chart` 资源库中部署 `Grafana`，覆盖一些默认的 `Chart` 值。请注意，`HelmChart` 资源本身在 `kube-system` 命名空间，但 `Chart` 资源将被部署到 `monitoring` 命名空间。

```
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: grafana
  namespace: kube-system
spec:
  chart: stable/grafana
  targetNamespace: monitoring
  set:
    adminPassword: "NotVerySafePassword"
  valuesContent: |-
    image:
      tag: master
    env:
      GF_EXPLORE_ENABLED: true
    adminUser: admin
    sidecar:
      datasources:
        enabled: true


```

[#](#k3s-高级选项) K3s 高级选项
-----------------------

**证书轮换**

默认情况下，`K3s` 的证书在 `12` 个月内过期。如果证书已经过期或剩余的时间不足 `90` 天，则在 `K3s` 重启时轮换证书。

**查询 k3s 证书过期时间：**

```
root@cubmaster01:/var/lib/rancher/k3s/server/manifests
>   do \
>     echo $i;\
>     openssl x509 -enddate -noout -in $i; \
>   done
/var/lib/rancher/k3s/server/tls/client-admin.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-ca.crt
notAfter=Nov 22 14:31:20 2032 GMT
/var/lib/rancher/k3s/server/tls/client-controller.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-k3s-cloud-controller.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-k3s-controller.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-kube-proxy.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/client-scheduler.crt
notAfter=Nov 25 14:31:20 2023 GMT
/var/lib/rancher/k3s/server/tls/request-header-ca.crt
notAfter=Nov 22 14:31:20 2032 GMT
/var/lib/rancher/k3s/server/tls/server-ca.crt
notAfter=Nov 22 14:31:20 2032 GMT
/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt
notAfter=Nov 25 14:31:20 2023 GMT


```

**修改和重启：**

```
timedatectl set-ntp no
date -s 20220807


service k3s restart


```

**Red Hat 和 CentOS 的额外准备**

建议运行以下命令，关闭 `firewalld` 防火墙。

```
sudo systemctl disable firewalld --now


```

[#](#所遇到的问题) 所遇到的问题
-------------------

*   关于 k3s issue
*   关于 讨论

提示

更多的节点和安装选择问题看下半部分（下一节）

> 1.master 节点的防火墙全部关掉，要不然 worker 可能连不上 master

```
# systemctl stop firewalld && systemctl disable firewalld && iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X


```

> 2. 有些. worker 可能连不上 master，可能有些 worker 的 iptables 表混乱了

```
# systemctl stop k3s-agent
# systemctl stop docker
# iptables -F && iptables -t nat -F  && iptables -t mangle -F
# systemctl start docker
# systemctl stop k3s-agent


```

> 3. 有些主机无法根据上面的命令下载 docker，提示证书过期

```
adding repo from: https://mydream.ink/utils/container/docker-ce.repo
grabbing file https://mydream.ink/utils/container/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
Could not fetch/save url https://mydream.ink/utils/container/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 14] curl#60 - "Peer's Certificate has expired."

因为这些主机的时间不正确，用ntp同步时间即可
# 安装ntp服务器
# yum install -y ntp
# 与一个已知的时间服务器同步
# ntpdate time.nist.gov
# 删除本地时间并设置时区为上海
# rm -rf /etc/localtime
# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime


```

> 4.master 配置 --write-kubeconfig

```
--write-kubeconfig ~/.kube/config效果为将配置文件写到k8s默认会用的位置，而不是k3s默认的位置/etc/rancher/k3s/k3s.yaml。后者会导致istio、helm需要额外设置或无法运行。


```

> 5. 命令补全

```
# yum install -y bash-completion
# source /usr/share/bash-completion/bash_completion
# source <(kubectl completion bash)
# echo "source <(kubectl completion bash)" >> ~/.bashrc


```

> 6. 提示需要安装相关软件，根据提示进行安装

```
# yum install -y k3s-selinux-0.1.1-rc1.el7.noarch.rpm
# yum install -y container-selinux selinux-policy-base


```

[#](#end-链接) END 链接
-------------------

about k3s

*   K3s 中文文档 - 国外：https://mirror.rancher.cn/
    
*   K3s 中文文档 - 国内：https://docs.rancher.cn/k3s/
    
*   K3s 国内镜像站 - 加速：https://mirror.rancher.cn/
    
*   K3s 系列教程 - 官方制作：https://github.com/kingsd041/k3s-tutorial
    

**代码地址：**

*   K3s 仓库地址 - Github：https://github.com/k3s-io

**扩展 (k3d)：**

*   [vscode-k3dopen in new window](https://github.com/inercia/vscode-k3d/)：VSCode 扩展，用于处理 VSCode 中的 k3d 集群
*   [k3xopen in new window](https://github.com/inercia/k3x)：k3d 的图形接口（用于 Linux）。
*   [AbsaOSS/k3d-actionopen in new window](https://github.com/AbsaOSS/k3d-action)：完全可定制的 GitHub Action 来运行轻量级 Kubernetes 集群。
*   [AutoK3sopen in new window](https://github.com/cnrancher/autok3s)：一个轻量级工具，可以帮助在任何地方运行 K3s，包括 k3d provider。
*   [nolar/setup-k3d-k3sopen in new window](https://github.com/nolar/setup-k3d-k3s)：为 GitHub Actions 设置 K3d/K3s。

**周边项目：**

*   **K3s 周边项目 - k3os**：https://github.com/rancher/k3os
    
    完全基于 `K8S` 管理的轻量级操作系统
    
*   **K3s 周边项目 - autok3s**：https://github.com/cnrancher/autok3s
    
    用于简化 `K3s` 集群部署和管理的轻量级工具
    
    即在阿里云和 `aws` 等云服务器上面部署 `k3s`
    
*   **K3s 周边项目 - k3d**：https://github.com/cnrancher/autok3s
    
    可以在 `k3d` 创建容器化的 `k3s` 集群 k3d 是一个轻量级包装器，用于在 docker 中运行 [k3sopen in new window](https://github.com/rancher/k3s)（Rancher Lab 的最小 Kubernetes 发行版）
    
    可以使用容器在单台计算机上启动多节点 `k3s` 集群
    
*   **K3s 周边项目 - harvester**：https://github.com/harvester/harvester
    
    基于 `K8S` 构建的开源超融合基础架构 (`HCI`) 软件
    
    旨在替换 `vSphere` 和 `Nutanix` 的开源替代方案
    
*   **K3s 周边项目 - octopus**：https://github.com/cnrancher/octopus
    
    主要用于边缘计算相关
    
    用于 `K8S` 和 `k3s` 的轻量级云原生设备管理系统
    
    集群可以将边缘设备作为自定义 `k8s` 资源进行管理
    

**about：**

*   [Ⓜ️回到目录🏠](https://docker.nsddd.top/)
    
*   [**🫵参与贡献💞❤️‍🔥💖**open in new window](https://nsddd.top/archives/contributors))
    
*   ✴️版权声明 © ：本书所有内容遵循 [CC-BY-SA 3.0 协议（署名 - 相同方式共享）©open in new window](http://zh.wikipedia.org/wiki/Wikipedia:CC-by-sa-3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC)